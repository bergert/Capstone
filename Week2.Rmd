---
title: "Week2"
author: "Thomas Berger"
date: "April 13, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal is to analyze text-files. The ZIP-file contains 4 different languages; for this report we analyze the english language.

There are a total 3 english language text-files: blogs, news and twitter.

## Tasks

Tasks to accomplish

- Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
- Profanity filtering - removing profanity and other words you do not want to predict.

Tips, tricks, and hints

- Loading the data in. This dataset is fairly large. We emphasize that you don't necessarily need to load the entire dataset in to build your algorithms (see point 2 below). At least initially, you might want to use a smaller subset of the data. Reading in chunks or lines using R's readLines or scan functions can be useful. You can also loop over each line of text by embedding readLines within a for/while loop, but this may be slower than reading in large chunks at a time. Reading pieces of the file at a time will require the use of a file connection in R. For example, the following code could be used to read the first few lines of the English Twitter dataset:con <- file("en_US.twitter.txt", "r") readLines(con, 1) ## Read the first line of text readLines(con, 1) ## Read the next line of text readLines(con, 5) ## Read in the next 5 lines of text close(con) ## It's important to close the connection when you are done
See the ?connections help page for more information.

- Sampling. To reiterate, to build models you don't need to load in and use all of the data. Often relatively few randomly selected rows or chunks need to be included to get an accurate approximation to results that would be obtained using all the data. Remember your inference class and how a representative sample can be used to infer facts about a population. You might want to create a separate sub-sample dataset by reading in a random subset of the original data and writing it out to a separate file. That way, you can store the sample and not have to recreate it every time. You can use the rbinom function to "flip a biased coin" to determine whether you sample a line of text or not.


## Getting the data

```{r}
   # getting data
   zipURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
   zipFile <- "Coursera-SwiftKey.zip"
   if (!file.exists(zipFile)) {
	    # if not exists, download it
		download.file(zipURL, zipFile)
        write(date(), file = "Coursera-SwiftKey.log")
        unzip (zipFile)
   }
   
   # show files
   filePath <- file.path(getwd(),"final","en_US")
   list.files(filePath)
   blogFile <- file.path(filePath,"en_US.blogs.txt")
   newsFile <- file.path(filePath,"en_US.news.txt")
   twitterFile <- file.path(filePath,"en_US.twitter.txt")
```

```{r}
   # getting swaer word data
   swearURL <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
   swearFile <- "badWords.txt"
   if (!file.exists(swearFile)) {
	    # if not exists, download it
		download.file(swearURL, swearFile)
        write(date(), file = "badWords.log")
   }
```

```{r}
   # function to read all lines of a file
   getLines <- function(f) {
      con <- file(f,open="r")
      lines <- readLines(con)
      close(con)
      lines
   }
   
   # read data files
   blogData <- getLines(blogFile)
   newsData <- getLines(newsFile)
   twitterData <- getLines(twitterFile)
   swearData <- getLines(swearFile)
   
   # build up result
   details <- data.frame(file=list.files(filePath),
                         file_MB=c(
                             round(file.info(blogFile)$size / 1024 / 1024, 2),
                             round(file.info(newsFile)$size / 1024 / 1024, 2),
                             round(file.info(twitterFile)$size / 1024 / 1024, 2) ),
                         no_of_lines=c(length(blogData),
                                       length(newsData),
                                       length(twitterData) ),
                         max_chars=c(max(unlist(lapply(blogData, nchar))),
                             max(unlist(lapply(newsData, nchar))),
                             max(unlist(lapply(newsData, nchar))) )
                         )
   details
```

## Cleaning the data

```{r}

split_sentences <- function (text) {
  # split based on periods, exclamation or question marks
  # ignore dots in case of a.m. or p.m.
  # map \\n to new lines, and also <space>\<double-quote>
  result <- unlist(strsplit (text, split = "((\\\")|[!?]|(\\n))|(\\.[^0-9m])"))

  # remaove spaces at start and end
  result <- lapply(result, function(x) gsub("^\\s+|\\s+$","",x))
  
  # remove empty lines
  result <- result [nchar (result) > 0]

  # always return something
  if (length (result) == 0)
      result <- ""

  return (result)
}

   # testing the split into sentences
   # text <- 'Colin Murphy. \"Boxing was resumed! why? To clear 4.20 at 5 p.m. Nice job'
   # strsplit (text, split = "[[\\.!?\n][\\\"]]+")

   set.seed(1234)
   library(tm)
   # get a sample of 3000 lines lines
   SampleData <-paste(sample(blogData, 3000, replace=FALSE), 
                      sample(newsData, 3000, replace=FALSE), 
                      sample(twitterData, 3000, replace=FALSE))
 
   # split sample data into sentences
   SampleData <- split_sentences(SampleData)

   # create the sample corpus 
   sample_corp<-Corpus(VectorSource(SampleData), readerControl=list(language="lat"))
   # perform cleaning operations
   sample_corp<-tm_map(sample_corp, PlainTextDocument)
   sample_corp<-tm_map(sample_corp, removePunctuation)
   sample_corp<-tm_map(sample_corp, stripWhitespace)
   sample_corp<-tm_map(sample_corp, tolower)
   sample_corp<-tm_map(sample_corp, removeNumbers)
   sample_corp<-tm_map(sample_corp, stemDocument)
   sample_corp<-tm_map(sample_corp,tolower)
   sample_corp<-tm_map(sample_corp, removeWords, stopwords("english"))
   sample_corp<-tm_map(sample_corp, removeWords, swearData)

```

## Create the N-gram

```{r}
```

## References

* [how to trim whitespaces](http://stackoverflow.com/questions/2261079/how-to-trim-leading-and-trailing-whitespace-in-r)
* [Text mining infrastructure in R](https://www.jstatsoft.org/article/view/v025i05)
* [CRAN: Natural Language Processing](http://cran.r-project.org/web/views/NaturalLanguageProcessing.html)
* [Stanford Natural Language Processing MOOC](https://www.coursera.org/learn/nlp)
* [Wikipedia what is a n-gram](https://en.wikipedia.org/wiki/N-gram)
* [R n-gram guide](https://cran.r-project.org/web/packages/ngram/vignettes/ngram-guide.pdf)
* [profanity word list (not used, because too short)](http://www.bannedwordlist.com)
* [bad word list of reasonable size](https://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/)
